\documentclass[journal]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{biblatex}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{epstopdf}

\title{Image Categorization: Recent Advances in Deep Learning Techniques}
\author{Mike Odnis}
\date{May 2024}

\begin{document}

\maketitle

\begin{abstract}
In artificial intelligence (AI) and machine learning (ML), image categorization is a key task with applications ranging from autonomous driving to medical diagnostics. This research investigates how deep learning techniques have advanced recently to increase the precision of picture classification systems. Accurately classifying photos is a tough challenge because of background clutter, object orientations, and inconsistent illumination. I examine state-of-the-art approaches such as data augmentation, transfer learning, and convolutional neural networks (CNNs) by doing an extensive literature study of ten foundational studies in the field. My goal in doing this evaluation is to pinpoint important patterns, obstacles, and interesting avenues for further study to improve picture classification accuracy.
\end{abstract}

\textbf{Keywords:} Artificial Intelligence, Machine Learning, Image Categorization, Deep Learning, Precision, Picture Classification, Data Augmentation, Transfer Learning, Convolutional Neural Networks

\section{Introduction}
Accurate classification of images is essential in computer vision, with numerous real-world applications. Convolutional Neural Networks (CNNs) play a pivotal role in this domain, offering robust solutions for tasks such as image processing, classification, and segmentation. Notably, CNNs have demonstrated effectiveness in sequential text classifications, particularly for relatively short-length sentences. In a CNN model, a convolutional kernel is applied to the input image, represented by pixels, yielding convolved features. As the kernel slides through the input image, it functions as a filter, extracting relevant features and producing a reduced convolved feature matrix. In image classification tasks, CNNs analyze groups of adjacent pixels together, enabling them to discern patterns and structures within the image. Similarly, in textual analysis, CNNs can learn to interpret groups of adjacent words as meaningful phrases, enhancing the model's understanding of context. A notable distinction between CNNs and previous models, such as Deep Neural Networks (DNNs) and Recurrent Neural Networks (RNNs), lies in their architecture. CNNs incorporate convolutional layers, along with 1D convolutions in the case of text analysis, followed by a flattening layer before the output. This structural design allows CNNs to efficiently capture hierarchical features, making them well-suited for a wide range of classification tasks.

\section{Literature Review}
Object detection and classification have been extensively studied in computer vision. Prior research has focused on improving accuracy and robustness through novel architectures and training strategies (e.g., \cite{lee2020me}, \cite{tan20203d}). In medical imaging, deep learning models have been explored for disease diagnosis and classification, demonstrating promising results for the accurate identification of pathological conditions \cite{tan20203d}. Face recognition has garnered significant attention, particularly in security and surveillance applications. Recent advancements in deep learning have led to significant improvements in recognition accuracy, even under challenging conditions such as occlusion \cite{kumar2020occluded}. Attribute prediction and image classification have been central topics in multimedia research, with a growing emphasis on leveraging semantic attributes to improve classification performance \cite{abdulnabi2015multi}.

\section{Methodology}
Several studies explored in this review investigate deep learning architectures for object detection and classification. Here, we delve deeper into the methodologies employed in some of these studies, highlighting aspects that can be visualized and represented with formulas.

\subsection{ME R-CNN: Multi-Expert R-CNN for Object Detection \cite{lee2020me}}
This approach utilizes multiple processing pipelines, each specializing in handling specific object appearances. An Expert Assignment Network (EAN) then directs Regions of Interest (RoIs) to the most suitable pipeline for processing.

\subsubsection{Visualization}
The architecture of ME R-CNN can be visualized as a flowchart, with multiple processing pipelines branching out from a central point representing the input image. Arrows would indicate the flow of RoIs through the network, with the EAN acting as a decision node directing them to the appropriate pipeline.

\subsubsection{Mathematical process}
The size of a feature map is $7\times7\times512$.
Convolutional layer employs 512 kernels ($3\times3$ with 1 stride and 1 padding).
ReLU is applied which is followed by $7\times7$ max-pooling generating a $1\times1\times512$ output.
Assuming $f(x, W_{\text{EAN}})$ is the function which computes the output of the EAN with weight $W_{\text{EAN}}$ when given per-RoI feature max $x$. 
The expert assignment process being carried out by the EAN can then be formulated as shown.
EAN weights are optimized by minimizing the loss $L_{\text{EAN}}(\cdot)$ which intakes the EAN output generated by $f(\cdot)$ and the expert label vector $y$.
The expert label vector $y$ is constructed by concatenating the expert labels.
For the very first iteration of training, $ME_0$ and $EAN_0$ are used in place of $ME_{t-1}$ and $EAN_{t-1}$, respectively, which are acquired by 'Initialize ME and Conv-L' and 'Initialize EAN'.

\subsection{3D-GLCM CNN: A 3-Dimensional Gray-Level Co-Occurrence Matrix-Based CNN Model for Polyp Classification via CT Colonography \cite{tan20203d}}
This method incorporates gray-level co-occurrence matrices (GLCMs) to capture spatial relationships between pixels. A 3D-GLCM CNN model then leverages these features for accurate polyp classification in CT colonography images.

\subsubsection{Visualization}
The creation of GLCMs can be visualized as a mathematical operation applied to image patches. Formulas can be presented to illustrate how GLCMs capture spatial relationships between pixels by calculating co-occurrence probabilities of neighboring pixel intensities. The 3D-GLCM CNN architecture itself can be visualized as a typical CNN structure, with additional emphasis on the initial layers that extract GLCM features.

\subsubsection{Mathematical process}
Based on the gray-level images from Step 1, we calculated the GLCMs by counting the frequency of the pair of voxels with specific gray-level (or bin) values. In a 2D digital image, co-occurrence matrix (CM) is defined by the frequency of pixel-pairs in one image.

\subsection{Occluded Thermal Face Recognition Using Bag of CNN (BoCNN) \cite{kumar2020occluded}}
This approach tackles the challenge of occluded thermal face recognition by employing a Bag of CNN (BoCNN) framework. BoCNN combines pre-trained CNN models and utilizes fusion strategies to enhance recognition performance even when faces are partially obscured.

\subsubsection{BoCNN Architecture}
The proposed BoCNN model leverages a combination of pre-trained CNN architectures, including VGG-19, ResNet-50, ResNet-101, Inception-V3, and Inception-ResNetV2. These

 models were chosen due to their unique design philosophies and variations in depth.

\subsubsection{Mathematical process}
Suppose, we have $m$ samples from $S1$ to $Sm$ to be classified among $n$ different classes ranging from $C1$ to $Cn$, using $L$ classifiers ranging from $T1$ to $TL$ then the probabilistic score of sample $Si$ for belongingness to class $Cj$ by classifier $Tk$ can be $P_{Tk}(i, j)$.

\section{Results}
Lee et al. \cite{lee2020me} report significant improvements in object detection accuracy using ME R-CNN on standard datasets. Tan et al. \cite{tan20203d} demonstrate that their 3D-GLCM CNN model achieves high accuracy in polyp classification, outperforming other image-based CNN models. Kumar and Singh \cite{kumar2020occluded} showcase enhanced performance in occluded thermal face recognition with BoCNN, surpassing existing methods in terms of both accuracy and robustness. Abdulnabi et al. \cite{abdulnabi2015multi} report that their multi-task CNN model achieves state-of-the-art results in attribute prediction tasks compared to single-task classifiers.

\section{Discussion}
Lee et al. \cite{lee2020me} introduce a novel approach to object detection with ME R-CNN, achieving significant improvements in accuracy through the use of multiple experts and an Expert Assignment Network (EAN). Tan et al. \cite{tan20203d} demonstrate the effectiveness of incorporating GLCM-based features for polyp classification using their 3D-GLCM CNN model. Kumar and Singh's BoCNN \cite{kumar2020occluded} effectively addresses the challenge of occluded thermal face recognition, achieving superior performance compared to existing methods. Abdulnabi et al. \cite{abdulnabi2015multi} showcase the efficacy of integrating semantic attributes for image classification tasks with their multi-task CNN model.

\section{Conclusion}
The research conducted sheds light on significant advancements in image classification facilitated by deep learning techniques, particularly Convolutional Neural Networks (CNNs). Key findings from the reviewed studies underscore the effectiveness of innovative approaches in addressing challenges associated with object detection and classification. These findings collectively emphasize the promising avenues for further research and development in image classification, with deep learning techniques offering considerable potential for advancing the precision and robustness of classification systems across various domains.

\begin{thebibliography}{9}
  \bibitem{lee2020me}
  H. Lee, S. Eum and H. Kwon, `ME R-CNN: Multi-Expert R-CNN for Object Detection,` in IEEE Transactions on Image Processing, vol. 29, pp. 1030-1044, 2020, doi: 10.1109/TIP.2019.2938879.
  
  \bibitem{tan20203d}
  J. Tan et al., `3D-GLCM CNN: A 3-Dimensional Gray-Level Co-Occurrence Matrix-Based CNN Model for Polyp Classification via CT Colonography,` in IEEE Transactions on Medical Imaging, vol. 39, no. 6, pp. 2013-2024, June 2020, doi: 10.1109/TMI.2019.2963177.
  
  \bibitem{kumar2020occluded}
  S. Kumar and S. K. Singh, `Occluded Thermal Face Recognition Using Bag of CNN ($Bo$CNN),` in IEEE Signal Processing Letters, vol. 27, pp. 975-979, 2020, doi: 10.1109/LSP.2020.2996429.
  
  \bibitem{abdulnabi2015multi}
  A. H. Abdulnabi, G. Wang, J. Lu and K. Jia, `Multi-Task CNN Model for Attribute Prediction,` in IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 1949-1959, Nov. 2015, doi: 10.1109/TMM.2015.2477680.
  
  \bibitem{zhang2021spectral}
  X. Zhang et al., `Spectral–Spatial Fractal Residual Convolutional Neural Network With Data Balance Augmentation for Hyperspectral Classification,` in IEEE Transactions on Geoscience and Remote Sensing, vol. 59, no. 12, pp. 10473-10487, Dec. 2021, doi: 10.1109/TGRS.2020.3046840.
  
  \bibitem{wu2018light}
  X. Wu, R. He, Z. Sun and T. Tan, `A Light CNN for Deep Face Representation With Noisy Labels,` in IEEE Transactions on Information Forensics and Security, vol. 13, no. 11, pp. 2884-2896, Nov. 2018, doi: 10.1109/TIFS.2018.2833032.
  
  \bibitem{bauerle2021net2vis}
  A. Bäuerle, C. van Onzenoodt and T. Ropinski, `Net2Vis – A Visual Grammar for Automatically Generating Publication-Tailored CNN Architecture Visualizations,` in IEEE Transactions on Visualization and Computer Graphics, vol. 27, no. 6, pp. 2980-2991, 1 June 2021, doi: 10.1109/TVCG.2021.3057483.
  
  \bibitem{cozzolino2020noiseprint}
  D. Cozzolino and L. Verdoliva, `Noiseprint: A CNN-Based Camera Model Fingerprint,` in IEEE Transactions on Information Forensics and Security, vol. 15, pp. 144-159, 2020, doi: 10.1109/TIFS.2019.2916364.
  
  \bibitem{krishnendu2020review}
  Krishnendu S, Geetha S, Gopakumar G, 2020, A Review on Polyp Detection and Segmentation in Colonoscopy Images using Deep Learning, INTERNATIONAL JOURNAL OF ENGINEERING RESEARCH & TECHNOLOGY (IJERT) Volume 09, Issue 10 (October 2020).
  \end{thebibliography}
  

\end{document}